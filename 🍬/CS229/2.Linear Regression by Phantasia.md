## Linear Regression

为了使我们的房屋示例更有趣，我们考虑一个更丰富的数据集，其中我们还知道每栋房屋的卧室数量：

| 居住面积（平方英尺） | 卧室数量 | 价格（千美元） |
| -------------------- | -------- | -------------- |
| 2104                 | 3        | 400            |
| 1600                 | 3        | 330            |
| 2400                 | 3        | 369            |
| 1416                 | 2        | 232            |
| 3000                 | 4        | 540            |
| ......               | ......   | ......         |

这里，x 是 $R^2 $ 空间中的二维向量。例如，$x_1^{(i)} $ 是训练集中第 i 个房屋的居住面积，而 $x_2^{(i)} $是其卧室数量。（通常，在设计学习问题时，由你来决定选择哪些特征，因此，如果你在波特兰收集住房数据，你也可能决定包括其他特征，例如每栋房屋是否有壁炉、浴室的数量等等。稍后我们会进一步讨论特征选择，但现在我们假设特征是已给定的。）

为了进行监督学习，我们必须决定如何在计算机中表示函数/假设 \( h \)。作为初步选择，我们假设 $y$ 是 $ x $ 的线性函数：
$$
h_\theta(x) = \theta_0 + \theta_1 x_1 + \theta_2 x_2
$$
这里，$ \theta_i $ 是参数（也称为权重），用于参数化从 $X $ 映射到 $Y$的线性函数空间。在不引起混淆的情况下，我们将省略 $h_\theta(x)$ 中的 $\theta$下标，并更简单地写成  $h(x)$。为了简化我们的符号，我们还引入了让 $ x_0 = 1 $ 的约定（这是截距项），这样：
$$
h(x) = \sum_{i=0}^d \theta_i x_i = \theta^T x
$$

这里，我们将 $ \theta $ 和 $ x $ 都视为向量，并且 $ d $ 是输入变量的数量（不包括 $ x_0 $​）。这里已经类似矩阵乘法了。

现在，给定一个训练集，我们如何选择或学习参数 $\theta $？一个合理的方法似乎是使 $h(x)$尽可能接近 $y$，至少对于我们已有的训练示例。为了形式化这一点，我们将定义一个函数，该函数衡量对于每个 $ \theta $ 值，$ h(x^{(i)}) $ 与相应的 $ y^{(i)} $ 的接近程度。我们定义成本函数：
$$
J(\theta) = \frac{1}{2} \sum_{i=1}^n (h_\theta(x^{(i)}) - y^{(i)})^2
$$


如果你以前见过线性回归，你可能会认出这是普通最小二乘回归模型产生的熟悉的最小二乘成本函数。无论你是否见过它，让我们继续前进，最终我们会展示这是一个更广泛算法家族的特殊情况。

$$
\mathbb{R}
$$


#### 1.1 LMS算法

我们希望选择 $\theta$ 以最小化 $J(\theta)$。为此，我们使用一个搜索算法，该算法从某个 $\theta$ 的“初始猜测”开始，并反复更改 $\theta$ 以使 $J(\theta)$ 变小，直到收敛到一个使 $J(\theta)$ 最小的 $\theta$ 值。具体来说，我们考虑梯度下降算法，该算法从某个初始 $\theta$ 开始，并反复执行更新：
$$
\theta_j := \theta_j - \alpha \frac{\partial}{\partial \theta_j} J(\theta)
$$


（此更新同时对 $j = 0, \ldots, d$ 的所有值执行。）这里，$\alpha$ 被称为学习率。这是一个非常自然的算法，它不断朝着 $J$ 最陡下降的方向迈出一步。

在LMS（最小均方）算法中，偏导数 $\frac{\partial}{\partial \theta_j} J(\theta)$ 的含义是损失函数 $J(\theta)$ 相对于参数 $\theta_j$的变化率。具体来说，偏导数衡量的是，当 $\theta_j$ 发生微小变化时，损失函数 $J(\theta)$ 会如何变化。

**偏导数 $\frac{\partial}{\partial \theta_j} J(\theta)$**：

- 这是 $J(\theta)$  对 $\theta_j$的偏导数，表示在其他参数保持不变的情况下，$\theta_j$ 的一个微小变化对损失函数 $J(\theta)$ 影响的大小和方向。
- 如果偏导数为正，说明增加 $\theta_j$ 会导致 $J(\theta)$  增大，因此我们需要减少 $\theta_j$ 以减小损失函数。
- 如果偏导数为负，说明增加$\theta_j$ 会导致 $J(\theta)$  减小，因此我们需要增加 $\theta_j$ 以减小损失函数。

#### 举例说明

假设我们有一个简单的线性回归模型，其损失函数是均方误差（MSE）：

$$
 J(\theta) = \frac{1}{2m} \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})^2 
$$
其中：

- $m$  是样本数量。
- $h_\theta(x^{(i)})$  是模型的预测值。
- $y^{(i)}$  是实际值。

为了更新参数 $\theta_j$，我们计算  $J(\theta)$  对 $\theta_j$ 的偏导数：

$$
\frac{\partial}{\partial \theta_j} J(\theta) = \frac{1}{m} \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)}) \cdot x_j^{(i)}
$$
这里，$(h_\theta(x^{(i)}) - y^{(i)})$ 是预测误差， $x_j^{(i)}$  是输入特征值。

通过计算这个偏导数，我们可以知道当前参数 $\theta_j$对整体损失的影响，从而决定下一步应该如何调整 $\theta_j$以最小化损失函数。

为了实现此算法，我们必须计算右侧的偏导数项。首先，让我们考虑只有一个训练示例 $(x, y)$ 的情况，这样我们可以忽略 $J$ 定义中的求和项。我们有：
$$
\frac{\partial}{\partial \theta_j} J(\theta) = \frac{\partial}{\partial \theta_j} \frac{1}{2} (h_\theta(x) - y)^2 \\
\quad \quad \quad \quad \quad  \quad \quad= (h_\theta(x) - y) \frac{\partial}{\partial \theta_j} (h_\theta(x) - y) \\
\quad \quad\quad \quad\quad \quad \quad \quad \quad= (h_\theta(x) - y) \frac{\partial}{\partial \theta_j} \left( \sum_{i=0}^d \theta_i x_i - y \right) \\
\quad \quad = (h_\theta(x) - y) x_j
$$
对于单个训练示例，这给出了更新规则：
$$
\theta_j := \theta_j + \alpha (y^{(i)} - h_\theta(x^{(i)})) x_j^{(i)}
$$
这个规则称为LMS更新规则（LMS代表“最小均方”），也称为Widrow-Hoff学习规则。这个规则有几个看起来自然和直观的特性。例如，更新的幅度与误差项 $(y^{(i)} - h_\theta(x^{(i)}))$ 成正比；因此，例如，如果我们遇到一个预测几乎匹配实际值 $y^{(i)}$ 的训练示例，那么我们会发现几乎不需要更改参数；相反，如果我们的预测 $h_\theta(x^{(i)})$ 有很大的误差（即，与 $y^{(i)}$ 相距甚远），将会对参数进行较大的更改。

我们推导出了当只有一个训练示例时的LMS规则。有两种方法可以将这种方法修改为多个训练示例的训练集。首先是将其替换为以下算法：

$
\begin{aligned}
&\qquad 重复直到收敛 \{ \\
&\qquad\qquad\theta_j := \theta_j + \alpha \sum^m_{i=1}(y^{(i)}-h_\theta (x^{(i)}))x_j^{(i)}\quad(对每个j) \\
&\qquad\}
\end{aligned}
$

通过将坐标更新分组为向量 $\theta$ 的更新，我们可以稍微简洁地重写更新：
$$
\theta := \theta + \alpha \sum_{i=1}^n (y^{(i)} - h_\theta(x^{(i)})) x^{(i)}
$$
n是 feature的数量，读者可以很容易地验证，上述更新规则中的求和量就是原始成本函数 $J$ 的偏导数 $\frac{\partial J(\theta)}{\partial \theta_j}$。所以，这就是对原始成本函数 $J$ 的梯度下降。该方法在每一步中查看整个训练集中的每个示例，称为批量梯度下降。请注意，尽管梯度下降通常容易受到局部最小值的影响，但我们在这里提出的线性回归优化问题只有一个全局最小值，没有其他局部最小值；因此梯度下降总是收敛（假设学习率 $\alpha$ 不太大）到全局最小值。实际上，$J$ 是一个凸二次函数。

以下是运行梯度下降以最小化二次函数的示例：

![image-20240621105938018](https://raw.githubusercontent.com/MarchPhantasia/pic/main/hexoblog/image-20240621105938018.png)

上图中的椭圆是二次函数的轮廓。还显示了梯度下降的轨迹，它在（48,30）初始化。图中的 $x$（由直线连接）标记了梯度下降经过的 $\theta$ 的连续值。

当我们运行批量梯度下降以拟合我们之前数据集上的 $\theta$，学习预测房价与居住面积的关系时，我们得到：
$$
\theta_0 = 71.27, \theta_1 = 0.1345
$$
如果我们将 $h_\theta(x)$ 作为 $x$（面积）的函数绘制出来，以及训练数据，我们得到如下图：

![image-20240621105951163](https://raw.githubusercontent.com/MarchPhantasia/pic/main/hexoblog/image-20240621105951163.png)



如果将卧室数量也作为输入特征之一，我们得到：
$$
\theta_0 = 89.60, \theta_1 = 0.1392, \theta_2 = -8.738
$$
上述结果是通过批量梯度下降获得的。还有一种替代批量梯度下降的方法也非常有效。考虑以下算法：

$
\begin{aligned}
&\qquad循环：\{ \\
&\qquad\qquad i从1到m,\{   \\
&\qquad\qquad\qquad\theta_j := \theta_j  +\alpha(y^{(i)}-h_{\theta}(x^{(i)}))x_j^{(i)} \qquad(对每个 j) \\
&\qquad\qquad\}  \\
&\qquad\}
\end{aligned}
$​

通过将坐标更新分组为向量 $\theta$ 的更新，我们可以稍微简洁地重写更新：
$$
 \theta := \theta + \alpha (y^{(i)} - h_\theta(x^{(i)})) x^{(i)} 
$$
在这个算法中，我们反复运行训练集，每次遇到一个训练示例时，我们根据仅对该单个训练示例的误差的梯度更新参数。这个算法称为随机梯度下降（也称为增量梯度下降）。而批量梯度下降在采取一步之前必须扫描整个训练集——如果 $n$ 很大，这是一个耗时的操作——随机梯度下降可以立即开始取得进展，并在每次查看示例时继续取得进展。通常，随机梯度下降比批量梯度下降更快地使 $\theta$“接近”最小值。（但请注意，它可能永远不会“收敛”到最小值，参数 $\theta$ 将围绕 $J(\theta)$​ 的最小值波动；但在实践中，大多数靠近最小值的值将是对真实最小值的合理近似。）出于这些原因，尤其是在训练集很大的情况下，通常更倾向于使用随机梯度下降。

#### 2 法方程（The normal equations）

上文中的梯度下降法是一种找出 $J$ 最小值的办法。然后咱们聊一聊另一种实现方法，这种方法寻找起来简单明了，而且不需要使用迭代算法。这种方法就是，我们直接利用找对应导数为 $0$ 位置的 $\theta_j$，这样就能找到 $J$ 的最小值了。我们想实现这个目的，还不想写一大堆代数公式或者好几页的矩阵积分，所以就要介绍一些做矩阵积分的记号。


##### 2.1 矩阵导数（Matrix derivatives）

假如有一个函数 $f: R^{m\times n} → R$ 从 $m\times n$ 大小的矩阵映射到实数域，那么就可以定义当矩阵为 $A$ 的时候有导函数 $f$ 如下所示：

$$
\nabla_A f(A)=\begin{bmatrix} \frac {\partial f}{\partial A_{11}} & \dots  & \frac {\partial f}{\partial A_{1n}} \\ \vdots  & \ddots & \vdots  \\ \frac {\partial f}{\partial A_{m1}} & \dots  & \frac {\partial f}{\partial A_{mn}} \\ \end{bmatrix}
$$

因此，这个梯度 $\nabla_A f(A)$本身也是一个 $m\times n$ 的矩阵，其中的第 $(i,j)$ 个元素是 $\frac {\partial f}{\partial A_{ij}} $ 。
假如 $ A =\begin{bmatrix} A_{11} & A_{12} \\ A_{21} & A_{22} \\ \end{bmatrix} $ 是一个 $2\times 2$  矩阵，然后给定的函数 $f:R^{2\times 2} → R$ 为:

$$
f(A) = \frac 32A_{11}+5A^2_{12}+A_{21}A_{22}
$$

这里面的 $A_{ij}$ 表示的意思是矩阵 $A$ 的第 $(i,j)$ 个元素。然后就有了梯度：

$$
\nabla _A f(A) =\begin{bmatrix} \frac  32 & 10A_{12} \\ A_{22} & A_{21} \\ \end{bmatrix}
$$

然后咱们还要引入 **$trace$** 求迹运算，简写为 $“tr”$。对于一个给定的 $n\times n$ 方形矩阵 $A$，它的迹定义为对角项和：

$$
trA = \sum^n_{i=1} A_{ii}
$$

假如 $a$ 是一个实数，实际上 $a$ 就可以看做是一个 $1\times 1$ 的矩阵，那么就有 $a$ 的迹 $tr a = a$。(如果你之前没有见到过这个“运算记号”，就可以把 $A$ 的迹看成是 $tr(A)$，或者理解成为一个对矩阵 $A$ 进行操作的 $trace$ 函数。不过通常情况都是写成不带括号的形式更多一些。) 

如果有两个矩阵 $A$ 和$B$，能够满足 $AB$ 为方阵，$trace$ 求迹运算就有一个特殊的性质： $trAB = trBA$ (自己想办法证明)。在此基础上进行推论，就能得到类似下面这样的等式关系：

$$
trABC=trCAB=trBCA \\
trABCD=trDABC=trCDAB=trBCDA
$$

下面这些和求迹运算相关的等量关系也很容易证明。其中 $A$ 和 $B$ 都是方形矩阵，$a$ 是一个实数：

$$
trA=trA^T \\
tr(A+B)=trA+trB \\
tr a A=a trA
$$

接下来咱们就来在不进行证明的情况下提出一些矩阵导数（其中的一些直到本节末尾才用得上）。另外要注意等式$(4)$中的$A$ 必须是**非奇异方阵（non-singular square matrices**），而 $|A|$ 表示的是矩阵 $A$ 的行列式。那么我们就有下面这些等量关系：

$$
\begin{aligned}
   \nabla_A tr AB & = B^T & \text{(1)}\\
   \nabla_{A^T} f(A) & = (\nabla_{A} f(A))^T &\text{(2)}\\
   \nabla_A tr ABA^TC& = CAB+C^TAB^T &\text{(3)}\\
   \nabla_A|A| & = |A|(A^{-1})^T &\text{(4)}\\
\end{aligned}
$$

为了让咱们的矩阵运算记号更加具体，咱们就详细解释一下这些等式中的第一个。假如我们有一个确定的矩阵 $B \in R^{n\times m}$（注意顺序，是$n\times m$，这里的意思也就是 $B$ 的元素都是实数，$B$ 的形状是 $n\times m$ 的一个矩阵），那么接下来就可以定义一个函数$ f: R^{m\times n} → R$ ，对应这里的就是 $f(A) = trAB$。这里要注意，这个矩阵是有意义的，因为如果 $A \in R^{m\times n} $，那么 $AB$ 就是一个方阵，是方阵就可以应用 $trace$ 求迹运算；因此，实际上 $f$ 映射的是从 $R^{m\times n} $ 到实数域 $R$。这样接下来就可以使用矩阵导数来找到 $\nabla_Af(A)$ ，这个导函数本身也是一个 $m \times n $的矩阵。上面的等式$(1)$ 表明了这个导数矩阵的第 $(i,j)$个元素等同于 $B^T$ （$B$的转置）的第 $(i,j)$ 个元素，或者更直接表示成 $B_{ji}$。

上面等式$(1-3)$ 都很简单，证明就都留给读者做练习了。等式$(4)$需要用逆矩阵的伴随矩阵来推导出。$^3$

>3 假如咱们定义一个矩阵 $A'$，它的第 $(i,j)$ 个元素是$ (−1)^{i+j}$ 与矩阵 $A $移除 第 $i$ 行 和 第 $j$ 列 之后的行列式的乘积，则可以证明有$A^{−1} = (A')^T /|A|$。（你可以检查一下，比如在 $A$ 是一个 $2\times 2$ 矩阵的情况下看看 $A^{-1}$ 是什么样的，然后以此类推。如果你想看看对于这一类结果的证明，可以参考一本中级或者高级的线性代数教材，比如Charles Curtis, 1991, Linear Algebra, Springer。）这也就意味着 $A' = |A|(A^{−1})^T $。此外，一个矩阵 $A$ 的行列式也可以写成 $|A| = \sum_j A_{ij}A'_{ij}$ 。因为 $(A')_{ij}$ 不依赖 $A_{ij}$ （通过定义也能看出来），这也就意味着$(\frac  \partial {\partial A_{ij}})|A| = A'_{ij} $，综合起来也就得到上面的这个结果了。

##### 2.2 最小二乘法回顾（Least squares revisited）

通过刚才的内容，咱们大概掌握了矩阵导数这一工具，接下来咱们就继续用逼近模型（closed-form）来找到能让 $J(\theta)$ 最小的 $\theta$ 值。首先咱们把 $J$ 用矩阵-向量的记号来重新表述。

给定一个训练集，把**设计矩阵（design matrix）** $x$ 设置为一个 $m\times n$ 矩阵（实际上，如果考虑到截距项，也就是 $\theta_0$ 那一项，就应该是 $m\times (n+1)$ 矩阵），这个矩阵里面包含了训练样本的输入值作为每一行：

$$
X =\begin{bmatrix}
-(x^{(1)}) ^T-\\
-(x^{(2)}) ^T-\\
\vdots \\
-(x^{(m)}) ^T-\\
\end{bmatrix} 
$$

然后，咱们设 $\vec{y}$ 是一个 $m$ 维向量（m-dimensional vector），其中包含了训练集中的所有目标值：

$$
y =\begin{bmatrix}
y^{(1)}\\
y^{(2)}\\
\vdots \\
y^{(m)}\\
\end{bmatrix}
$$

因为 $h_\theta (x^{(i)}) = (x^{(i)})^T\theta $`译者注：这个怎么推出来的我目前还没尝试，目测不难`，所以可以证明存在下面这种等量关系：

$$
\begin{aligned}
X\theta - \vec{y}  &=
\begin{bmatrix}
(x^{(1)})^T\theta \\
\vdots \\
(x^{(m)})^T\theta\\
\end{bmatrix} -
\begin{bmatrix}
y^{(1)}\\
\vdots \\
y^{(m)}\\
\end{bmatrix}\\
& =
\begin{bmatrix}
h_\theta (x^{1}) -y^{(1)}\\
\vdots \\
h_\theta (x^{m})-y^{(m)}\\
\end{bmatrix}\\
\end{aligned}
$$

对于向量 $\vec{z}$ ，则有 $z^T z = \sum_i z_i^2$ ，因此利用这个性质，可以推出:

$$
\begin{aligned}
\frac 12(X\theta - \vec{y})^T (X\theta - \vec{y}) &=\frac 12 \sum^m_{i=1}(h_\theta (x^{(i)})-y^{(i)})^2\\
&= J(\theta)
\end{aligned}
$$

最后，要让 $J$ 的值最小，就要找到函数对于$\theta$导数。结合等式$(2)$和等式$(3)$，就能得到下面这个等式$(5)$：

$$
\nabla_{A^T} trABA^TC =B^TA^TC^T+BA^TC \qquad \text{(5)}
$$

因此就有：

$$
\begin{aligned}
\nabla_\theta J(\theta) &= \nabla_\theta \frac 12 (X\theta - \vec{y})^T (X\theta - \vec{y}) \\
&= \frac  12 \nabla_\theta (\theta ^TX^TX\theta -\theta^T X^T \vec{y} - \vec{y} ^TX\theta +\vec{y}^T \vec{y})\\
&= \frac  12 \nabla_\theta tr(\theta ^TX^TX\theta -\theta^T X^T \vec{y} - \vec{y} ^TX\theta +\vec{y}^T \vec{y})\\
&= \frac  12 \nabla_\theta (tr \theta ^TX^TX\theta - 2tr\vec{y} ^T X\theta)\\
&= \frac  12 (X^TX\theta+X^TX\theta-2X^T\vec{y}) \\
&= X^TX\theta-X^T\vec{y}\\
\end{aligned}
$$

在第三步，我们用到了一个定理，也就是一个实数的迹就是这个实数本身；第四步用到了 $trA = trA^T$ 这个定理；第五步用到了等式$(5)$，其中 $A^T =\theta, B=B^T =X^TX, C=I$,还用到了等式 $(1)$。要让 $J$ 取得最小值，就设导数为 $0$ ，然后就得到了下面的**法线方程（normal equations）：**

$$
X^TX\theta =X^T\vec{y}
$$

所以让 $J(\theta)$ 取值最小的 $\theta$ 就是

$$
\theta = (X^TX)^{-1}X^T\vec{y}
$$

